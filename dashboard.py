import streamlit as st
import torch
import torch.nn as nn
import json
import re
import time

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´
# ==========================================
st.set_page_config(
    page_title="HDFS Log Anomaly Detector",
    page_icon="üõ°Ô∏è",
    layout="wide"
)

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
MODEL_PATH = 'lstm_model_weights.pt'
VOCAB_PATH = 'event_to_int_vocab.json'
LOG_FILE_PATH = 'HDFS.log'


# ==========================================
# 2. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ú–û–î–ï–õ–ò (–û–ë–ù–û–í–õ–ï–ù–ù–û–ï –ü–û–î –ù–û–í–£–Æ –¢–†–ï–ù–ò–†–û–í–ö–£)
# ==========================================
class LSTMClassifier(nn.Module):
    # –î–æ–±–∞–≤–∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä dropout=0.5, –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    def __init__(self, vocab_size, emb_dim=64, hid_dim=128, out_dim=2, n_layers=2, dropout=0.5):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)

        # –í–∞–∂–Ω–æ: –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±—ã–ª dropout, —Ç—É—Ç –æ–Ω —Ç–æ–∂–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏,
        # —Ö–æ—Ç—è –≤ —Ä–µ–∂–∏–º–µ eval() –æ–Ω –æ—Ç–∫–ª—é—á–∏—Ç—Å—è —Å–∞–º.
        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,
                            batch_first=True, dropout=dropout)

        self.fc = nn.Linear(hid_dim, out_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        out = hidden[-1]
        out = self.dropout(out)  # –°–ª–æ–π –µ—Å—Ç—å, –Ω–æ –≤ eval() –æ–Ω –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç –¥–∞–Ω–Ω—ã–µ
        return self.fc(out)


# ==========================================
# 3. –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–†
# ==========================================
class HDFSLogParser:
    def __init__(self):
        self.blk_pattern = re.compile(r"(blk_[-0-9]+)")
        self.signatures = {
            "Receiving block": "E2",
            "Received block": "E22",
            "PacketResponder": "E5",
            "Served block": "E3",
            "verification succeeded": "E26",
            "addStoredBlock": "E11",
            "allocateBlock": "E9",
            "Deleting block": "E25",
            "ask": "E27",
            "Exception": "E_Error",
            "warn": "E_Warn"
        }

    def parse_line(self, line):
        match = self.blk_pattern.search(line)
        if not match: return None, None
        block_id = match.group(1)
        event_type = "Unknown"
        for key, eid in self.signatures.items():
            if key in line:
                event_type = eid
                break
        return block_id, event_type


# ==========================================
# 4. –§–£–ù–ö–¶–ò–ò –ó–ê–ì–†–£–ó–ö–ò
# ==========================================
@st.cache_resource
def load_resources():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    try:
        with open(VOCAB_PATH, 'r') as f:
            vocab = json.load(f)
    except FileNotFoundError:
        st.error(f"–§–∞–π–ª {VOCAB_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        st.stop()

    vocab_size = len(vocab) + 1

    # === –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ó–î–ï–°–¨ ===
    # –ë—ã–ª–æ: 100, –°—Ç–∞–ª–æ: 64 (—Ç–∞–∫ –∫–∞–∫ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—á–∏–ª–∞—Å—å —Å emb_dim=64)
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: vocab_size, emb_dim=64, hid_dim=128, out_dim=2, n_layers=2
    model = LSTMClassifier(vocab_size, emb_dim=64, hid_dim=128, out_dim=2, n_layers=2).to(device)

    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
        model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (–≤—ã–∫–ª—é—á–∞–µ—Ç Dropout)
    except FileNotFoundError:
        st.error(f"–§–∞–π–ª –≤–µ—Å–æ–≤ {MODEL_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        st.stop()
    except RuntimeError as e:
        st.error(f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏! –£–¥–∞–ª–∏—Ç–µ —Å—Ç–∞—Ä—ã–π .pt —Ñ–∞–π–ª –∏ —Å–∫–∞—á–∞–π—Ç–µ –Ω–æ–≤—ã–π. –û—à–∏–±–∫–∞: {e}")
        st.stop()

    return model, vocab, device


# ==========================================
# 5. –ò–ù–¢–ï–†–§–ï–ô–°
# ==========================================
model, vocab, device = load_resources()
parser = HDFSLogParser()

st.sidebar.title("‚öôÔ∏è –ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
st.sidebar.success(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. Device: {device}")
speed = st.sidebar.slider("–°–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è –ª–æ–≥–æ–≤ (—Å–µ–∫)", 0.01, 1.0, 0.1)

st.title("üõ°Ô∏è AI Log Sentinel")
st.markdown("–°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –ª–æ–≥–∞—Ö HDFS –Ω–∞ –±–∞–∑–µ **LSTM**.")

tab1, tab2 = st.tabs(["üì° –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ—Ç–æ–∫–∞ (Live)", "üîç –†—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"])

# --- –ü–û–¢–û–ö ---
with tab1:
    col1, col2 = st.columns([3, 1])
    with col1:
        st.subheader("–ü–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HDFS.log")
    with col2:
        start_btn = st.button("‚ñ∂ –ó–ê–ü–£–°–¢–ò–¢–¨ –ü–û–¢–û–ö")

    m1, m2 = st.columns(2)
    metric_ok = m1.empty()
    metric_anom = m2.empty()
    log_container = st.container(height=300, border=True)

    if 'count_ok' not in st.session_state: st.session_state.count_ok = 0
    if 'count_anom' not in st.session_state: st.session_state.count_anom = 0

    if start_btn:
        active_sessions = {}
        try:
            with open(LOG_FILE_PATH, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if not line: continue

                    blk_id, event_str = parser.parse_line(line)
                    if not blk_id or event_str == "Unknown": continue

                    event_idx = vocab.get(event_str, 0)
                    if blk_id not in active_sessions: active_sessions[blk_id] = []
                    active_sessions[blk_id].append(event_idx)

                    # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê –¢–†–ò–ì–ì–ï–†–ê (–ë–ï–ó E5) ---
                    should_predict = False
                    if event_str in ["E26", "E25"]:
                        should_predict = True
                    elif len(active_sessions[blk_id]) > 40:
                        should_predict = True

                    if should_predict:
                        sequence = active_sessions[blk_id]
                        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–±—Ä—ã–≤–∫–∏, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —è–≤–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
                        if len(sequence) < 3 and event_str != "E26":
                            del active_sessions[blk_id]
                            continue

                        tensor = torch.tensor([sequence], dtype=torch.long).to(device)

                        # –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï (–ß–ò–°–¢–û–ï, –ë–ï–ó –ö–û–°–¢–´–õ–ï–ô)
                        is_anomaly = False
                        with torch.no_grad():
                            out = model(tensor)
                            probs = torch.softmax(out, dim=1)
                            confidence = probs[0][1].item()
                            _, pred = torch.max(out, 1)
                            if pred.item() == 1: is_anomaly = True

                        if is_anomaly:
                            st.session_state.count_anom += 1
                            log_container.error(
                                f"üö® –ê–ù–û–ú–ê–õ–ò–Ø! Block: {blk_id} | Len: {len(sequence)} | Conf: {confidence:.2%}")
                        else:
                            st.session_state.count_ok += 1
                            msg = "‚úÖ Verified" if event_str == "E26" else "‚ÑπÔ∏è Ends"
                            log_container.success(f"{msg}: {blk_id} | Len: {len(sequence)}")

                        del active_sessions[blk_id]
                        metric_ok.metric("–ù–æ—Ä–º–∞", st.session_state.count_ok)
                        metric_anom.metric("–ê–Ω–æ–º–∞–ª–∏–∏", st.session_state.count_anom)
                        time.sleep(speed)

        except FileNotFoundError:
            st.error("–§–∞–π–ª HDFS.log –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# --- –†–£–ß–ù–û–ô –í–í–û–î ---
with tab2:
    st.header("–†—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑")
    st.info("–í–≤–µ–¥–∏—Ç–µ –∫–æ–¥—ã —Å–æ–±—ã—Ç–∏–π —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª.")

    # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - –Ω–æ—Ä–º–∞
    user_input = st.text_input("–°–æ–±—ã—Ç–∏—è:", "E5 E22 E11 E9 E11 E9 E26 E26 E26")

    # –ü–æ–ª–∑—É–Ω–æ–∫ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Ñ–∏—á–∞ –¥–ª—è –∑–∞—â–∏—Ç—ã!)
    threshold = st.slider("–ü–æ—Ä–æ–≥ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è —Ç—Ä–µ–≤–æ–≥–∏ (%)", 1, 100, 15) / 100.0

    if st.button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å"):
        tokens = user_input.strip().split()
        numeric_seq = [vocab[t] for t in tokens if t in vocab]

        if not numeric_seq:
            st.error("–ù–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π")
        else:
            tensor = torch.tensor([numeric_seq], dtype=torch.long).to(device)
            with torch.no_grad():
                out = model(tensor)
                probs = torch.softmax(out, dim=1)
                prob_anom = probs[0][1].item()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ (0.0 - 1.0)

            # === –õ–û–ì–ò–ö–ê –ü–†–ò–ù–Ø–¢–ò–Ø –†–ï–®–ï–ù–ò–Ø ===
            is_anomaly = False
            reason = ""

            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ù–µ–π—Ä–æ—Å–µ—Ç–∏ (—Å —É—á–µ—Ç–æ–º –ø–æ—Ä–æ–≥–∞)
            if prob_anom > threshold:
                is_anomaly = True
                # reason = f"–ù–µ–π—Ä–æ—Å–µ—Ç—å –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω (Risk > {int(threshold * 100)}%)"

            # 2. –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–ó–∞—â–∏—Ç–∞ –æ—Ç E5 E5 E5...)
            # –ï—Å–ª–∏ –≤ —Ü–µ–ø–æ—á–∫–µ —Ç–æ–ª—å–∫–æ 1 –∏–ª–∏ 2 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è, –Ω–æ –¥–ª–∏–Ω–∞ –±–æ–ª—å—à–∞—è - —ç—Ç–æ —Å–ø–∞–º/DOS
            # unique_events = len(set(numeric_seq))
            # if len(numeric_seq) > 5 and unique_events < 2:
            #     is_anomaly = True
            #     prob_anom = 0.99  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ–º —Ä–∏—Å–∫ –¥–ª—è UI
                # reason = "–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π (DoS –ø–∞—Ç—Ç–µ—Ä–Ω)"

            # === –í–´–í–û–î ===
            st.metric("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–∏ (AI)", f"{prob_anom:.2%}")

            if is_anomaly:
                st.error(f"–†–ï–ó–£–õ–¨–¢–ê–¢: üõë –û–ë–ù–ê–†–£–ñ–ï–ù–ê –£–ì–†–û–ó–ê")
                st.warning(f"–ü—Ä–∏—á–∏–Ω–∞: {reason}")
            else:
                st.success("–†–ï–ó–£–õ–¨–¢–ê–¢: ‚úÖ –í–°–Å –ß–ò–°–¢–û")